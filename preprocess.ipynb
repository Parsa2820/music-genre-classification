{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "print(\"Step 1 completed: Necessary libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for audio preprocessing\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "N_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "NUM_SEGMENTS = 5\n",
    "\n",
    "print(\"Step 2 completed: Preprocessing parameters defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the data structure\n",
    "def initialize_data_structure():\n",
    "    \"\"\"\n",
    "    Initialize the structure to store MFCCs and labels.\n",
    "    :return: A dictionary to store mapping, MFCC features, and labels.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mapping\": [],  # List of genres\n",
    "        \"mfcc\": [],  # List of MFCC feature matrices\n",
    "        \"labels\": []  # List of corresponding labels\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "data = initialize_data_structure()\n",
    "print(\"Step 3 completed: Data structure initialized.\")\n",
    "print(data)  # Print the initialized structure for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def save_mfcc_to_npy(dataset_path, output_dir, num_segments=5):\n",
    "    \"\"\"\n",
    "    Extracts MFCCs from audio dataset and saves them into .npy files.\n",
    "\n",
    "    :param dataset_path: Path to the dataset containing subfolders for each genre.\n",
    "    :param output_dir: Directory to save the extracted MFCCs and labels as .npy files.\n",
    "    :param num_segments: Number of segments to divide each track into.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store features and labels\n",
    "    all_mfccs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Calculate the number of samples per segment\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / HOP_LENGTH)\n",
    "\n",
    "    # List to keep track of failed files\n",
    "    failed_files = []\n",
    "\n",
    "    # Walk through all subdirectories and files in the dataset\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        # Ensure we are not at the root directory\n",
    "        if dirpath != dataset_path:\n",
    "            # Extract the genre label (subfolder name)\n",
    "            genre_label = os.path.basename(dirpath)\n",
    "            print(f\"Processing genre: {genre_label}\")\n",
    "\n",
    "            # Process each file in the genre subdirectory\n",
    "            for file_name in filenames:\n",
    "                file_path = os.path.join(dirpath, file_name)\n",
    "\n",
    "                try:\n",
    "                    # Load the audio file\n",
    "                    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "                    # Ensure the audio file is long enough\n",
    "                    if len(signal) >= SAMPLES_PER_TRACK:\n",
    "                        # Process each segment\n",
    "                        for s in range(num_segments):\n",
    "                            start_sample = samples_per_segment * s\n",
    "                            end_sample = start_sample + samples_per_segment\n",
    "\n",
    "                            try:\n",
    "                                # Extract MFCC for the segment\n",
    "                                mfcc = librosa.feature.mfcc(\n",
    "                                    y=signal[start_sample:end_sample],\n",
    "                                    sr=sr,\n",
    "                                    n_mfcc=N_MFCC,\n",
    "                                    n_fft=N_FFT,\n",
    "                                    hop_length=HOP_LENGTH\n",
    "                                )\n",
    "                                mfcc = mfcc.T  # Transpose to have time steps as rows\n",
    "\n",
    "                                # Ensure MFCC matrix has the expected size\n",
    "                                if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "                                    all_mfccs.append(mfcc)\n",
    "                                    all_labels.append(i - 1)  # Subtract 1 for zero-based indexing\n",
    "                                    print(f\"Processed file: {file_name}, segment: {s+1}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipped segment {s+1} of file {file_name}: Unexpected MFCC shape.\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing segment {s+1} of file {file_name}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Skipped {file_name}: Audio too short.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    failed_files.append(file_path)\n",
    "                    continue\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    all_mfccs = np.array(all_mfccs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save as .npy files\n",
    "    np.save(os.path.join(output_dir, \"mfccs.npy\"), all_mfccs)\n",
    "    np.save(os.path.join(output_dir, \"labels.npy\"), all_labels)\n",
    "\n",
    "    print(f\"MFCCs and labels saved to {output_dir}\")\n",
    "\n",
    "    # Log failed files\n",
    "    if failed_files:\n",
    "        failed_log_path = os.path.join(output_dir, \"failed_files.txt\")\n",
    "        with open(failed_log_path, \"w\") as log_file:\n",
    "            for failed_file in failed_files:\n",
    "                log_file.write(f\"{failed_file}\\n\")\n",
    "        print(f\"Failed files log saved to {failed_log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_path = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Data\\genres_original\"  \n",
    "output_dir = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\"  \n",
    "\n",
    "# Extract and save MFCCs to .npy files\n",
    "save_mfcc_to_npy(dataset_path, output_dir, num_segments=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Ensure the NumPy module is correctly imported\n",
    "import os  # Used for file and directory operations\n",
    "\n",
    "# Load preprocessed data\n",
    "X = np.load(r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\\mfccs.npy\")  # Original MFCC features\n",
    "y = np.load(r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\\labels.npy\")  # Original labels\n",
    "\n",
    "print(f\"Loaded dataset: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "\n",
    "# Save the unaugmented original data\n",
    "original_output_dir = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\"\n",
    "os.makedirs(original_output_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(original_output_dir, \"X_original.npy\"), X)\n",
    "np.save(os.path.join(original_output_dir, \"y_original.npy\"), y)\n",
    "\n",
    "print(f\"Original dataset saved to: {original_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import librosa\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "print(\"Step 1 completed: Necessary libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for audio preprocessing\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "N_MFCC = 13\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "NUM_SEGMENTS = 5\n",
    "\n",
    "print(\"Step 2 completed: Preprocessing parameters defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the data structure\n",
    "def initialize_data_structure():\n",
    "    \"\"\"\n",
    "    Initialize the structure to store MFCCs and labels.\n",
    "    :return: A dictionary to store mapping, MFCC features, and labels.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mapping\": [],  # List of genres\n",
    "        \"mfcc\": [],  # List of MFCC feature matrices\n",
    "        \"labels\": []  # List of corresponding labels\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "data = initialize_data_structure()\n",
    "print(\"Step 3 completed: Data structure initialized.\")\n",
    "print(data)  # Print the initialized structure for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def save_mfcc_to_npy(dataset_path, output_dir, num_segments=5):\n",
    "    \"\"\"\n",
    "    Extracts MFCCs from audio dataset and saves them into .npy files.\n",
    "\n",
    "    :param dataset_path: Path to the dataset containing subfolders for each genre.\n",
    "    :param output_dir: Directory to save the extracted MFCCs and labels as .npy files.\n",
    "    :param num_segments: Number of segments to divide each track into.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store features and labels\n",
    "    all_mfccs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Calculate the number of samples per segment\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / HOP_LENGTH)\n",
    "\n",
    "    # List to keep track of failed files\n",
    "    failed_files = []\n",
    "\n",
    "    # Walk through all subdirectories and files in the dataset\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        # Ensure we are not at the root directory\n",
    "        if dirpath != dataset_path:\n",
    "            # Extract the genre label (subfolder name)\n",
    "            genre_label = os.path.basename(dirpath)\n",
    "            print(f\"Processing genre: {genre_label}\")\n",
    "\n",
    "            # Process each file in the genre subdirectory\n",
    "            for file_name in filenames:\n",
    "                file_path = os.path.join(dirpath, file_name)\n",
    "\n",
    "                try:\n",
    "                    # Load the audio file\n",
    "                    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "                    # Ensure the audio file is long enough\n",
    "                    if len(signal) >= SAMPLES_PER_TRACK:\n",
    "                        # Process each segment\n",
    "                        for s in range(num_segments):\n",
    "                            start_sample = samples_per_segment * s\n",
    "                            end_sample = start_sample + samples_per_segment\n",
    "\n",
    "                            try:\n",
    "                                # Extract MFCC for the segment\n",
    "                                mfcc = librosa.feature.mfcc(\n",
    "                                    y=signal[start_sample:end_sample],\n",
    "                                    sr=sr,\n",
    "                                    n_mfcc=N_MFCC,\n",
    "                                    n_fft=N_FFT,\n",
    "                                    hop_length=HOP_LENGTH\n",
    "                                )\n",
    "                                mfcc = mfcc.T  # Transpose to have time steps as rows\n",
    "\n",
    "                                # Ensure MFCC matrix has the expected size\n",
    "                                if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "                                    all_mfccs.append(mfcc)\n",
    "                                    all_labels.append(i - 1)  # Subtract 1 for zero-based indexing\n",
    "                                    print(f\"Processed file: {file_name}, segment: {s+1}\")\n",
    "                                else:\n",
    "                                    print(f\"Skipped segment {s+1} of file {file_name}: Unexpected MFCC shape.\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing segment {s+1} of file {file_name}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Skipped {file_name}: Audio too short.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    failed_files.append(file_path)\n",
    "                    continue\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    all_mfccs = np.array(all_mfccs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save as .npy files\n",
    "    np.save(os.path.join(output_dir, \"mfccs.npy\"), all_mfccs)\n",
    "    np.save(os.path.join(output_dir, \"labels.npy\"), all_labels)\n",
    "\n",
    "    print(f\"MFCCs and labels saved to {output_dir}\")\n",
    "\n",
    "    # Log failed files\n",
    "    if failed_files:\n",
    "        failed_log_path = os.path.join(output_dir, \"failed_files.txt\")\n",
    "        with open(failed_log_path, \"w\") as log_file:\n",
    "            for failed_file in failed_files:\n",
    "                log_file.write(f\"{failed_file}\\n\")\n",
    "        print(f\"Failed files log saved to {failed_log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_path = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Data\\genres_original\"  \n",
    "output_dir = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\"  \n",
    "\n",
    "# Extract and save MFCCs to .npy files\n",
    "save_mfcc_to_npy(dataset_path, output_dir, num_segments=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Time Shifting\n",
    "def time_shift(mfcc, shift=10):\n",
    "    return np.roll(mfcc, shift, axis=0)  \n",
    "\n",
    "# 2. Frequency Masking\n",
    "def frequency_mask(mfcc, mask_factor=5):\n",
    "    mfcc = mfcc.copy()\n",
    "    freq_start = np.random.randint(0, mfcc.shape[1] - mask_factor)\n",
    "    mfcc[:, freq_start:freq_start + mask_factor] = 0\n",
    "    return mfcc\n",
    "\n",
    "# 3. Time Masking\n",
    "def time_mask(mfcc, mask_factor=5):\n",
    "    mfcc = mfcc.copy()\n",
    "    time_start = np.random.randint(0, mfcc.shape[0] - mask_factor)\n",
    "    mfcc[time_start:time_start + mask_factor, :] = 0\n",
    "    return mfcc\n",
    "\n",
    "# 4. Add Noise\n",
    "def add_noise(mfcc, noise_factor=0.01):\n",
    "    noise = noise_factor * np.random.randn(*mfcc.shape)\n",
    "    return mfcc + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Data Augmentation Methods\n",
    "augmentations = [time_shift, frequency_mask, time_mask, add_noise]\n",
    "\n",
    "# Load preprocessed data (.npy files)\n",
    "mfccs_path = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\\mfccs.npy\"\n",
    "labels_path = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\\labels.npy\"\n",
    "\n",
    "X = np.load(mfccs_path)  # Load MFCC features\n",
    "y = np.load(labels_path)  # Load labels\n",
    "\n",
    "# Confirm the data shape\n",
    "print(f\"Loaded dataset: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "\n",
    "# Expand the dataset\n",
    "augmented_X = []\n",
    "augmented_y = []\n",
    "\n",
    "for i, sample in enumerate(X):\n",
    "    # Original sample\n",
    "    augmented_X.append(sample)\n",
    "    augmented_y.append(y[i])\n",
    "\n",
    "    # Augmented samples (generate 2 augmented versions for each sample)\n",
    "    for _ in range(2):\n",
    "        augmented_sample = sample\n",
    "        augmentation = np.random.choice(augmentations)\n",
    "        augmented_sample = augmentation(augmented_sample)\n",
    "        augmented_X.append(augmented_sample)\n",
    "        augmented_y.append(y[i])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "augmented_X = np.array(augmented_X)\n",
    "augmented_y = np.array(augmented_y)\n",
    "\n",
    "print(f\"Original dataset size: {X.shape}\")\n",
    "print(f\"Augmented dataset size: {augmented_X.shape}\")\n",
    "\n",
    "# Ensure the augmented dataset dimensions are correct\n",
    "assert augmented_X.shape[0] == X.shape[0] * 3, \"Enhanced dataset size mismatch.\"\n",
    "assert augmented_y.shape[0] == y.shape[0] * 3, \"Enhanced label size mismatch.\"\n",
    "\n",
    "# Add a channel dimension to adapt to CNN input\n",
    "augmented_X = augmented_X[..., np.newaxis]\n",
    "\n",
    "# Print the final shape\n",
    "print(f\"Augmented dataset shape (with channel): {augmented_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the save path\n",
    "output_dir = r\"C:\\Users\\schnuller\\Desktop\\ECE1513H\\Project\\Saved NPY Files\"  # Custom save path\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the target directory exists\n",
    "\n",
    "# Save the augmented dataset\n",
    "np.save(os.path.join(output_dir, \"augmented_X.npy\"), augmented_X)\n",
    "np.save(os.path.join(output_dir, \"augmented_y.npy\"), augmented_y)\n",
    "\n",
    "print(f\"Augmented dataset saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
